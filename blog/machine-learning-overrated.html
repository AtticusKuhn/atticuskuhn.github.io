<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@site"/><meta name="twitter:creator" content="@handle"/><meta property="og:locale" content="en_IE"/><meta property="og:site_name" content="Atticus Kuhn&#x27;s Personal Website"/><link rel="icon" href="/images/logo.png"/><title>Atticus Kuhn | Machine Learning is overrated</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="I don&#x27;t understand the hype around machine learning"/><meta property="og:title" content="BLOG | Machine Learning is overrated"/><meta property="og:description" content="I don&#x27;t understand the hype around machine learning"/><meta property="og:url" content="https://atticuskuhn.github.io/blog/machine-learning-overrated"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2022-01-27T08:00:00.000Z"/><meta property="article:author" content="https://atticuskuhn.github.io"/><meta property="article:tag" content="machine learning"/><meta property="article:tag" content=" ml"/><meta property="article:tag" content=" AI"/><meta property="og:image" content="/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"/><link rel="icon" href="/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"/><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "Blog",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://atticuskuhn.github.io/blog/machine-learning-overrated"
    },
    "headline": "Machine Learning is overrated",
    "image": [
      "/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"
     ],
    "datePublished": "2022-01-27T08:00:00.000Z",
    "dateModified": "2022-01-27T08:00:00.000Z",
    "author": {"@type": "Person","name": "Atticus Kuhn"},
    "description": "I don't understand the hype around machine learning"
  }</script><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/cdba98c5db96be0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cdba98c5db96be0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-378e68e29c265886.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-e47bb435dabe4202.js" defer=""></script><script src="/_next/static/chunks/pages/_app-58f1f96156c7d808.js" defer=""></script><script src="/_next/static/chunks/159-b06dba1abeb7a615.js" defer=""></script><script src="/_next/static/chunks/233-799031fd804c406e.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-4063093e2fc7c341.js" defer=""></script><script src="/_next/static/SIfo-1bC1kRzNQzE18IOD/_buildManifest.js" defer=""></script><script src="/_next/static/SIfo-1bC1kRzNQzE18IOD/_ssgManifest.js" defer=""></script><script src="/_next/static/SIfo-1bC1kRzNQzE18IOD/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><link href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" rel="stylesheet"/><div class="light"><div class="text-center flex flex-col w-full  bg-primary-100 text-primary-800"><header class="text-xl"><nav class="px-1"><a class="mx-1 px-1" href="/">Home</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/blog">Blog</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/projects">My Projects</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/about">About</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/computer-science">Computer Science</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/math">Mathematics</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/contact">Contact me</a> <!-- -->|<!-- --> <button class="bg-button w-fit p-3 rounded m-2 text-center text-primary-300 duration-75 undefined duration-200 hover:bg-primary-700 disabled:hover:bg-parimary-200 disabled:cursor-not-allowed">light</button></nav></header><div class="center bg-primary-200 mx-200 text-center min-h-screen p-3xl "><div class="container mx-auto flex content-center flex-col my-3xl"><h1 class="flex flex-col text-primary-900 text-4xl font-bold p-1">Machine Learning is overrated</h1><div class="text-primary-400">I don&#x27;t understand the hype around machine learning</div><img alt="Machine Learning is overrated" title="Machine Learning is overrated" class="w-6/12 h-6/12 mx-auto" src="/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"/><div class="text-sm">By Atticus Kuhn</div><div class="text-sm">Published <!-- -->1/27/2022</div><div class="text-sm">Tags: <!-- -->machine learning,  ml,  AI</div></div><p class="text-justify my-3xl w-10/12 sm:w-8/12 mx-auto"><div class="markdown"><p>In recent years, we&#x27;ve seen a lot of hype around machine learning. From <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">alpha go</a>, which beat world-champion go
players, to <a href="https://en.wikipedia.org/wiki/GPT-3" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">GPT-3</a>, the constant news buzz
would make one think we are around the corner from <a href="https://en.wikipedia.org/wiki/Technological_singularity" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">AI Dominance</a>. But I claim today that most of AI is just hype, with no
real substance behind it. Of course time will tell the true use of AI. In fact, most
times teams eagerly throw a <a href="https://en.wikipedia.org/wiki/Neural_network" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">neural net</a> at
a problem, they are really looking for a symbolic manipulation tool like <a href="https://en.wikipedia.org/wiki/Prolog" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">prolog</a>.</p>
<h1 class="flex flex-col text-primary-900 text-3xl font-bold p-1">Black Boxes</h1>
<p>The first drawback of machine learning is they are incomprehensible. If a machine learning model
makes a mistake, how will you figure out what went wrong? There is no real way of knowing. Sure,
you can look at the internal state of the model, but what does that really tell you?
Models today are getting so large that it is impossible to hold a single one in your
brain. GPT-3 has <a href="https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/#:~:text=The%20largest%20version%20GPT%2D3,and%203.2%20M%20batch%20size.&amp;text=Shown%20in%20the%20figure%20above,that%20it%20is%20quite%20larger." node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">175 billion parameters</a>
parameters. What if it makes a mistake? Will some engineer have to go through all
175 billion parameters just to find it? I think the true solution is <a href="https://en.wikipedia.org/wiki/Declarative_programming" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">declarative</a> software
that simply states rules or definitions. That code is easier to debug and understand for
new users. In essence, we can only look at machine learning as a black box.</p>
<h1 class="flex flex-col text-primary-900 text-3xl font-bold p-1">Changing Requirements</h1>
<p>Often in software engineering, projects are subject to <a href="https://rebelsguidetopm.com/help-the-requirements-keep-changing-and-i-cant-nail-them-down-part-2/" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">changing requirements</a>.
What happens when this is done to a machine learning model? In short, I don&#x27;t
see how a machine learning model trained for one task can be fixed to work on
a slightly different task. It would require a whole new retraining of the model
every time you want to make a change. Google even spoke about this issue in their
paper <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Machine Learning:
The High-Interest Credit Card of Technical Debt</a>. In our ever-changing world, you must truly
consider if you want to use a technology that requires so much effort even to change
its purpose slightly.</p>
<h1 class="flex flex-col text-primary-900 text-3xl font-bold p-1">Declarative Programming: is it Nirvana?</h1>
<p>I am a huge fan of declarative programming, whether from the <a href="https://en.wikipedia.org/wiki/Wolfram_Language" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Wolfram Language</a> or from <a href="https://en.wikipedia.org/wiki/Prolog" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Prolog</a>.
In such a style, the programmer simply states the rules of the system formally, and
the language takes care of implementation concerns. This approach is easy to understand
because it is just the rules of the system. In addition, it is easy to understand
the behavoir of a program: just follow which rules it applied.  If you have never
heard of prolog, I strongly encourage you to check it out. It is unlike any other
programming language, not part of the C or ALGOL lineage. It will rethink the way
that you understand the purpose of computers, <a href="https://swish.swi-prolog.org/" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">so try it out</a>.</p></div></p><div><div class="text-lg font-bold"> Reccomended Articles</div><div class="fl"><a href="/blog/decidability-tradeoff"><div class="p-sm m-base flex flex-col bg-primary-100 rounded m-lg pop max-w-xl min-h-full "><img alt="The Decidability Tradeoff" class="p-tiny mx-auto m-0 block h-full w-full" src="/images/automata.jpeg"/><div class="font-bold">The Decidability Tradeoff</div><div class="text-primary-300 text-xs">There is a fine line between power and decidability</div></div></a><a href="/blog/javascript-bad"><div class="p-sm m-base flex flex-col bg-primary-100 rounded m-lg pop max-w-xl min-h-full "><img alt="Javascript Being Bad is why it&#x27;s Good" class="p-tiny mx-auto m-0 block h-full w-full" src="/images/techs/javascript.png"/><div class="font-bold">Javascript Being Bad is why it&#x27;s Good</div><div class="text-primary-300 text-xs">People malign javascript for all the wrong reasons</div></div></a><a href="/blog/ross"><div class="p-sm m-base flex flex-col bg-primary-100 rounded m-lg pop max-w-xl min-h-full "><img alt="A Review of the Ross Program" class="p-tiny mx-auto m-0 block h-full w-full" src="/images/ross-logo.svg"/><div class="font-bold">A Review of the Ross Program</div><div class="text-primary-300 text-xs">I just attended the Ross Program</div></div></a></div></div></div><footer><hr/><span>Ⓒ all rights reserved <a href="https://atticuskuhn.github.io" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Atticus Kuhn</a> <!-- -->2023</span></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"blog":{"content":"\nIn recent years, we've seen a lot of hype around machine learning. From [alpha go](https://deepmind.com/research/case-studies/alphago-the-story-so-far), which beat world-champion go\nplayers, to [GPT-3](https://en.wikipedia.org/wiki/GPT-3), the constant news buzz\nwould make one think we are around the corner from [AI Dominance](https://en.wikipedia.org/wiki/Technological_singularity). But I claim today that most of AI is just hype, with no\nreal substance behind it. Of course time will tell the true use of AI. In fact, most\ntimes teams eagerly throw a [neural net](https://en.wikipedia.org/wiki/Neural_network) at\na problem, they are really looking for a symbolic manipulation tool like [prolog](https://en.wikipedia.org/wiki/Prolog).\n\n# Black Boxes\n\nThe first drawback of machine learning is they are incomprehensible. If a machine learning model\nmakes a mistake, how will you figure out what went wrong? There is no real way of knowing. Sure,\nyou can look at the internal state of the model, but what does that really tell you?\nModels today are getting so large that it is impossible to hold a single one in your\nbrain. GPT-3 has [175 billion parameters](https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/#:~:text=The%20largest%20version%20GPT%2D3,and%203.2%20M%20batch%20size.\u0026text=Shown%20in%20the%20figure%20above,that%20it%20is%20quite%20larger.)\nparameters. What if it makes a mistake? Will some engineer have to go through all\n175 billion parameters just to find it? I think the true solution is [declarative](https://en.wikipedia.org/wiki/Declarative_programming) software\nthat simply states rules or definitions. That code is easier to debug and understand for\nnew users. In essence, we can only look at machine learning as a black box.\n\n# Changing Requirements\n\nOften in software engineering, projects are subject to [changing requirements](https://rebelsguidetopm.com/help-the-requirements-keep-changing-and-i-cant-nail-them-down-part-2/).\nWhat happens when this is done to a machine learning model? In short, I don't\nsee how a machine learning model trained for one task can be fixed to work on\na slightly different task. It would require a whole new retraining of the model\nevery time you want to make a change. Google even spoke about this issue in their\npaper [Machine Learning:\nThe High-Interest Credit Card of Technical Debt](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf). In our ever-changing world, you must truly\nconsider if you want to use a technology that requires so much effort even to change\nits purpose slightly.\n\n# Declarative Programming: is it Nirvana?\n\nI am a huge fan of declarative programming, whether from the [Wolfram Language](https://en.wikipedia.org/wiki/Wolfram_Language) or from [Prolog](https://en.wikipedia.org/wiki/Prolog).\nIn such a style, the programmer simply states the rules of the system formally, and\nthe language takes care of implementation concerns. This approach is easy to understand\nbecause it is just the rules of the system. In addition, it is easy to understand\nthe behavoir of a program: just follow which rules it applied.  If you have never\nheard of prolog, I strongly encourage you to check it out. It is unlike any other\nprogramming language, not part of the C or ALGOL lineage. It will rethink the way\nthat you understand the purpose of computers, [so try it out](https://swish.swi-prolog.org/).","date":"January 27, 2022","tags":["machine learning"," ml"," AI"],"title":"Machine Learning is overrated","slug":"machine-learning-overrated","description":"I don't understand the hype around machine learning","image":"/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"},"reccomendedBlog":[{"content":"\n\n# What is Undecidability\n\nLike Icarus flying too close to the sun it seems that every mathematical system\nwill eventually be drawn towards the black hole of undecidability. Decidability\nbasically means there is an algorithm to answer a question. [Adding Numbers](https://lisbdnet.com/what-is-standard-algorithm-addition/#:~:text=standard%20algorithm%20addition%3F-,The%20standard%20algorithm%20for%20addition%20has%20three%20simple%20rules%3A,3%3A%20Regroup%2C%20if%20necessary) is decidable but [finding the shortest path](https://en.wikipedia.org/wiki/Travelling_salesman_problem) is not. In general I have noticed a pattern in\nall systems of rules. The system starts off simple. Then, as more use cases pile up, more\nrules and properties are added until suddenly, the system is undecidable. What happend here?\nI will start with a few examples\n\n# Typing Systems for Programming Languages\n\nI first encountered this problem in the type system of programming languages, or basically\nan algorithm that says `5` is an integer or `[1,2,3]` is a list of integers. You may think this problem is easy, and indeed the way I have just phrased it is decidable by [Algorithm W](https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system#Algorithm_W). A type system is very useful,\nbut people have encountered corner cases, such as depenedent types or linear types, and they\ntry to extend the type system. For example, if you are trying to figure out the type of \n[C](https://en.wikipedia.org/wiki/The_C_Programming_Language)'s [printf](https://en.wikipedia.org/wiki/Printf_format_string), you might realise that the type of the second\nargument depends on the type of the first argument.\n\n```c\nprintf(\"%f %s\", 1.2, \"hello\")\n```\nIf you want to encode this information into the type system, then you have just discovered\ndependent types, where types depend on values. The problem of finding out if a dependently\ntyped program is valid is actually undecidable (as a subset of the [halting problem](https://en.wikipedia.org/wiki/Halting_problem)). One you say you want this, you have stepped out\nof the safety of decidable typing and you might now need to prove to the compiler\nusing a formal proof that a program typchecks, such as in languages such as [Idris](https://www.idris-lang.org/). A good documentation of this problem is on the website [typing is hard](https://3fx.ch/typing-is-hard.html).\n\nIn my view, the ideal type system of a programming language should be decidable. I don't want\nto have to write a proof that my progam is valid. The best type system for this is [Hindley-Milner](https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system) which is about as far as you can creep before you plumet off the cliff of undeciability.\n\n# Presbuger Arithmetic\n\nAnother beautiful example of walking the tightrope of decidability is [Presburger Arithmetic](https://en.wikipedia.org/wiki/Presburger_arithmetic). Surprisingly, it is a formulation of arithmetic\nwhere every statement is decidable by a simple algorithm. For example, you can say \n`x+y \u003e 10` and the Presburger algorithm will find all satisfying x and y. What is the catch?\nThere is no multiplication. Presbuger Arithmetic only has plus, equals, there exists, and for all.\nSo statements about prime factorization, for example, are impossible to state in Presburger Arithmetic. It seems that multiplcation is intricately linked with undecidability. And\nOnce we say we want multiplication, arithmetic no longer becomes decidable.\n\n# Undecidability is Interesting\n\nUnlike programmers, who would want decidabilty type systems, we mathematicians do **not**\nwant decidability in our systems. In truth, decidable systems are boring, and pose no\ninteresting mathematical questions. Due to the well-established relationship between\nexpressiveness and decidability, once we make a system expressive enough to pose\ninteresting questions, it becomes undecidable. Furthermore, in decidable systems,\ntruth is simply a matter of computation. If you want to check if a statement is \ntrue, just run a computer to check it. Mathematics in decidable systems loses\nall its beauty and elegance of unconventional proofs.  So maybe undeciability isn't so bad.\n\n# Walking off the Cliff\n\nWhat can you take away from this? Well let's say you're designing a programming tool like\na configuration language or a macro system. Users might demand more and more features, until,\nunwittingly, your simple little tool becomes undecidable. This happened to C++ templates,\nwhere now they are [turing complete](https://stackoverflow.com/questions/189172/c-templates-turing-complete) (or in other words undecidable). So my advice is to\nkeep in mind the original goal of your project and be aware of any features that\nmight cause you to wander from the island of safety that is decidability. \n\n\n","date":"April 7, 2022","tags":["automata"," decidability"," presbuger arithmetic"," type system"],"title":"The Decidability Tradeoff","slug":"decidability-tradeoff","description":"There is a fine line between power and decidability","image":"/images/automata.jpeg"},{"content":"\n\nMany people mock Javascript for being bad. Go on any programming forum and you will find countless jokes at the expense of Javascript, but I argue that many of the things people hate most about Javascript were necessary for it to survive.\nWhy is Javascript perceived as bad?\nTo see why this is the case let us first look at the most maligned aspects of Javascript. When I ask people why they hate Javascript, they will say something along the lines of \nweak typing\n\n\n# general weirdness\nI find many of the criticisms of javascript are \"why doesn't this language stop me from shooting myself in the foot\". These are certainly desirable traits, and they are certainly being remedied in modern times with strict mode in ES6 and typescript. \nThe History of Javascript \nLooking at the history of Javascript reveals why it must have been this way. It was invented by Brendan Eich in 10 days in 1995, when the web was nothing like it is today.  Just imagine how different the web is by looking at a website from 1995 and comparing it to any modern website. Eich, and indeed nobody, could have imagined how the web could have evolved, so planning for the web to change drastically was needed.\n\n# Opinionated vs Flexible\nThe core of my argument is that people want Javascript to be opinionated and strict, when, due to the complex and evolving nature of the web, it is necessary that javascript be flexible and weak. Let's say Javascript took a hard stand and became inflexible on an issue pertaining to the 90s. We today would find that anachronistic and annoying. No central authority can predict how a programming language can be used, so it's better to leave all tools available to the programmer instead of walling off some features for fear that they might be mishandled. \n","date":"May 26, 2021","tags":["javascript"," web"," web programming"],"title":"Javascript Being Bad is why it's Good","slug":"javascript-bad","description":"People malign javascript for all the wrong reasons","image":"/images/techs/javascript.png"},{"content":"\n\nI just recently got back from attending the [Ross Mathematics Program](https://rossprogram.org/) in Ohio and\nI would like to give a review. I came back from the airport giddy with excitement, and I wish that\nthe Ross Program could go for another week to distract me from school.\n\n## The Math\n\nOn the Ross Website, it makes it seem like you will do nothing but math for 6 weeks, and I can say that \nthe Ross Program is exactly as advertised. You will get 6 weeks of mathematics. The main curriculum in\nnumber theory was very interesting, and gave me a deeper appreciation of the integers. You may think\nthat proving that if a is divisible by b, then b \u003c a might be a trivial task, but  at the Ross Program,\nwe go **FULL RIGOR**, meaning that every proposition must be directly from the axioms with minimal\ninference or hand waving (the student's response: \"full rigor leads to rigor mortis\"). All the problem\nsets have interesting problems that test your conceptions of numbers. \n\n## The Counsellors\nAll the counsellors at the Ross Program are excited to help you with math and discuss any math topic (Shoutout to Jon!). Just say \"I have a math problem\" and 10 heads will snap in your direction. Problem sets are\ngraded daily by the counsellors. Being in a 4-person \"family\" with a consellor creates a sense\nof camaraderie, which is good because I arrived at Ross not knowing anyone.\n\n\n## Lectures\nSpecial lectures are given by the counsellors and professors from [OSU](https://www.osu.edu/). These are\nvery exciting and you do not want to miss them. They cover eclectic topics ranging from modular forms\nto computation theory. I made the mistake of missing out on Vitaly Berglesson's Pigeonhole lecture series in\nthe first week and I regret that I did not go. The lectures introduce you to new subjects and really broadened\nmy conception of what I thought math encompassed. My favourite talk might have been Oscar's talk on the [hyper-reals](https://en.wikipedia.org/wiki/Hyperreal_number)\n\n## The Community\nThe community was my favorite thing about Ross. It is one thing to solve interesting problems, but it\nis quite another to collaborate on problems with people who are just as obsessed with math. \nWe all sort of egged each other on to solve more problems and inspired each other. I feel\nthat the biggest advantage of the Ross Program is that **it prepares you for how life will be in\nuniversity**. I.E. in university, you will likely specialize in a subject and hang out with people\nin a similar major to discuss your shared passion. \n\n## The Food\nI'd rather not dwell on this subject for too long.\n\n\n## Tips\nIf you are going to the Ross Program, I have some tips for you.\n- learn how to use a washing machine before you go\n- bring some emergency food with you\n- **pro tip:** bring a sleeping bag because the dorm beds are unsatisfactory to say the least. If you bring \na sleeping bag, you will be able to sleep.\n- In a similar vein, bring ear-plugs, because there are some people in the next dorm over\nwho say up very late and you don't want to be kept awake\n- The Ross website may claim that you shouldn't bring phones/computers/board games/cards, but \nyou will probably be fine if you bring these things.\n\n\n## Should you go to Ross?\nIf you would like to do math for 10 hours a day, for 7 days a week, for 6 weeks straight, then apply \nto the Ross Program. I was a bit intimidated at first because I had never done that much math\nbefore, but I really liked the experience. ","date":"August 28, 2022","tags":["math"," math program"," Ross"," Ross program"],"title":"A Review of the Ross Program","slug":"ross","description":"I just attended the Ross Program","image":"/images/ross-logo.svg"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"machine-learning-overrated"},"buildId":"SIfo-1bC1kRzNQzE18IOD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>