<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@site"/><meta name="twitter:creator" content="@handle"/><meta property="og:locale" content="en_IE"/><meta property="og:site_name" content="Atticus Kuhn&#x27;s Personal Website"/><link rel="icon" href="/images/logo.png"/><title>Atticus Kuhn | Machine Learning is overrated</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="I don&#x27;t understand the hype around machine learning"/><meta property="og:title" content="BLOG | Machine Learning is overrated"/><meta property="og:description" content="I don&#x27;t understand the hype around machine learning"/><meta property="og:url" content="https://atticuskuhn.github.io/blog/machine-learning-overrated"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2022-01-27T08:00:00.000Z"/><meta property="article:author" content="https://atticuskuhn.github.io"/><meta property="article:tag" content="machine learning"/><meta property="article:tag" content=" ml"/><meta property="article:tag" content=" AI"/><meta property="og:image" content="/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"/><link rel="icon" href="/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"/><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "Blog",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://atticuskuhn.github.io/blog/machine-learning-overrated"
    },
    "headline": "Machine Learning is overrated",
    "image": [
      "/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"
     ],
    "datePublished": "2022-01-27T08:00:00.000Z",
    "dateModified": "2022-01-27T08:00:00.000Z",
    "author": {"@type": "Person","name": "Atticus Kuhn"},
    "description": "I don't understand the hype around machine learning"
  }</script><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/cdba98c5db96be0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cdba98c5db96be0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-378e68e29c265886.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-e47bb435dabe4202.js" defer=""></script><script src="/_next/static/chunks/pages/_app-58f1f96156c7d808.js" defer=""></script><script src="/_next/static/chunks/159-b06dba1abeb7a615.js" defer=""></script><script src="/_next/static/chunks/233-799031fd804c406e.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-5ed30fe38d8f9c22.js" defer=""></script><script src="/_next/static/_0_1wzyqbB4XLdIGrVw4s/_buildManifest.js" defer=""></script><script src="/_next/static/_0_1wzyqbB4XLdIGrVw4s/_ssgManifest.js" defer=""></script><script src="/_next/static/_0_1wzyqbB4XLdIGrVw4s/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><link href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" rel="stylesheet"/><div class="light"><div class="text-center flex flex-col w-full  bg-primary-100 text-primary-800"><header class="text-xl"><nav class="px-1"><a class="mx-1 px-1" href="/">Home</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/blog">Blog</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/projects">My Projects</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/about">About</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/computer-science">Computer Science</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/math">Mathematics</a> <!-- -->|<!-- --> <a class="mx-1 px-1" href="/contact">Contact me</a> <!-- -->|<!-- --> <button class="bg-button w-fit p-3 rounded m-2 text-center text-primary-300 duration-75 undefined duration-200 hover:bg-primary-700 disabled:hover:bg-parimary-200 disabled:cursor-not-allowed">light</button></nav></header><div class="center bg-primary-200 mx-200 text-center min-h-screen p-3xl "><div class="container mx-auto flex content-center flex-col my-3xl"><h1 class="flex flex-col text-primary-900 text-4xl font-bold p-1">Machine Learning is overrated</h1><div class="text-primary-400">I don&#x27;t understand the hype around machine learning</div><img alt="Machine Learning is overrated" title="Machine Learning is overrated" class="w-6/12 h-6/12 mx-auto" src="/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"/><div class="text-sm">By Atticus Kuhn</div><div class="text-sm">Published <!-- -->1/27/2022</div><div class="text-sm">Tags: <!-- -->machine learning,  ml,  AI</div></div><p class="text-justify my-3xl w-10/12 sm:w-8/12 mx-auto"><div class="markdown"><p>In recent years, we&#x27;ve seen a lot of hype around machine learning. From <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">alpha go</a>, which beat world-champion go
players, to <a href="https://en.wikipedia.org/wiki/GPT-3" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">GPT-3</a>, the constant news buzz
would make one think we are around the corner from <a href="https://en.wikipedia.org/wiki/Technological_singularity" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">AI Dominance</a>. But I claim today that most of AI is just hype, with no
real substance behind it. Of course time will tell the true use of AI. In fact, most
times teams eagerly throw a <a href="https://en.wikipedia.org/wiki/Neural_network" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">neural net</a> at
a problem, they are really looking for a symbolic manipulation tool like <a href="https://en.wikipedia.org/wiki/Prolog" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">prolog</a>.</p>
<h1 class="flex flex-col text-primary-900 text-3xl font-bold p-1">Black Boxes</h1>
<p>The first drawback of machine learning is they are incomprehensible. If a machine learning model
makes a mistake, how will you figure out what went wrong? There is no real way of knowing. Sure,
you can look at the internal state of the model, but what does that really tell you?
Models today are getting so large that it is impossible to hold a single one in your
brain. GPT-3 has <a href="https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/#:~:text=The%20largest%20version%20GPT%2D3,and%203.2%20M%20batch%20size.&amp;text=Shown%20in%20the%20figure%20above,that%20it%20is%20quite%20larger." node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">175 billion parameters</a>
parameters. What if it makes a mistake? Will some engineer have to go through all
175 billion parameters just to find it? I think the true solution is <a href="https://en.wikipedia.org/wiki/Declarative_programming" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">declarative</a> software
that simply states rules or definitions. That code is easier to debug and understand for
new users. In essence, we can only look at machine learning as a black box.</p>
<h1 class="flex flex-col text-primary-900 text-3xl font-bold p-1">Changing Requirements</h1>
<p>Often in software engineering, projects are subject to <a href="https://rebelsguidetopm.com/help-the-requirements-keep-changing-and-i-cant-nail-them-down-part-2/" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">changing requirements</a>.
What happens when this is done to a machine learning model? In short, I don&#x27;t
see how a machine learning model trained for one task can be fixed to work on
a slightly different task. It would require a whole new retraining of the model
every time you want to make a change. Google even spoke about this issue in their
paper <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Machine Learning:
The High-Interest Credit Card of Technical Debt</a>. In our ever-changing world, you must truly
consider if you want to use a technology that requires so much effort even to change
its purpose slightly.</p>
<h1 class="flex flex-col text-primary-900 text-3xl font-bold p-1">Declarative Programming: is it Nirvana?</h1>
<p>I am a huge fan of declarative programming, whether from the <a href="https://en.wikipedia.org/wiki/Wolfram_Language" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Wolfram Language</a> or from <a href="https://en.wikipedia.org/wiki/Prolog" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Prolog</a>.
In such a style, the programmer simply states the rules of the system formally, and
the language takes care of implementation concerns. This approach is easy to understand
because it is just the rules of the system. In addition, it is easy to understand
the behavoir of a program: just follow which rules it applied.  If you have never
heard of prolog, I strongly encourage you to check it out. It is unlike any other
programming language, not part of the C or ALGOL lineage. It will rethink the way
that you understand the purpose of computers, <a href="https://swish.swi-prolog.org/" node="[object Object]" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">so try it out</a>.</p></div></p><div><div class="text-lg font-bold"> Reccomended Articles</div><div class="fl"><a href="/blog/sandeep-interview"><div class="p-sm m-base flex flex-col bg-primary-100 rounded m-lg pop max-w-xl min-h-full "><img alt="Interview with Machine Learning Expert" class="p-tiny mx-auto m-0 block h-full w-full" src="/images/sandeep.jpg"/><div class="font-bold">Interview with Machine Learning Expert</div><div class="text-primary-300 text-xs">After his 30 years in the field, machine learning engineer Sandeep Srinivasan offers advice on how to tackle this complex career.</div></div></a><a href="/blog/promise-is-a-monad"><div class="p-sm m-base flex flex-col bg-primary-100 rounded m-lg pop max-w-xl min-h-full "><img alt="Why promise IS a monad" class="p-tiny mx-auto m-0 block h-full w-full" src="/images/monad.jpg"/><div class="font-bold">Why promise IS a monad</div><div class="text-primary-300 text-xs">There are monads all around us, even if we don&#x27;t notice it</div></div></a><a href="/blog/create-language"><div class="p-sm m-base flex flex-col bg-primary-100 rounded m-lg pop max-w-xl min-h-full "><img alt="How to Create an Excellent Programming Lanugage" class="p-tiny mx-auto m-0 block h-full w-full" src="/images/lisp.png"/><div class="font-bold">How to Create an Excellent Programming Lanugage</div><div class="text-primary-300 text-xs">I have come up with a method to design well-loved languages</div></div></a></div></div></div><footer><hr/><span><a href="rss.xml" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">RSS feed</a> |<a href="atom.xml" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">atom feed</a> |<a href="rss.json" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">JSON feed</a> | MIT License <a href="https://atticuskuhn.github.io" class="pop rounded bg-primary-300 p-1 m-1 hover:bg-primary-100">Atticus Kuhn</a> <!-- -->2023</span></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"blog":{"content":"\nIn recent years, we've seen a lot of hype around machine learning. From [alpha go](https://deepmind.com/research/case-studies/alphago-the-story-so-far), which beat world-champion go\nplayers, to [GPT-3](https://en.wikipedia.org/wiki/GPT-3), the constant news buzz\nwould make one think we are around the corner from [AI Dominance](https://en.wikipedia.org/wiki/Technological_singularity). But I claim today that most of AI is just hype, with no\nreal substance behind it. Of course time will tell the true use of AI. In fact, most\ntimes teams eagerly throw a [neural net](https://en.wikipedia.org/wiki/Neural_network) at\na problem, they are really looking for a symbolic manipulation tool like [prolog](https://en.wikipedia.org/wiki/Prolog).\n\n# Black Boxes\n\nThe first drawback of machine learning is they are incomprehensible. If a machine learning model\nmakes a mistake, how will you figure out what went wrong? There is no real way of knowing. Sure,\nyou can look at the internal state of the model, but what does that really tell you?\nModels today are getting so large that it is impossible to hold a single one in your\nbrain. GPT-3 has [175 billion parameters](https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/#:~:text=The%20largest%20version%20GPT%2D3,and%203.2%20M%20batch%20size.\u0026text=Shown%20in%20the%20figure%20above,that%20it%20is%20quite%20larger.)\nparameters. What if it makes a mistake? Will some engineer have to go through all\n175 billion parameters just to find it? I think the true solution is [declarative](https://en.wikipedia.org/wiki/Declarative_programming) software\nthat simply states rules or definitions. That code is easier to debug and understand for\nnew users. In essence, we can only look at machine learning as a black box.\n\n# Changing Requirements\n\nOften in software engineering, projects are subject to [changing requirements](https://rebelsguidetopm.com/help-the-requirements-keep-changing-and-i-cant-nail-them-down-part-2/).\nWhat happens when this is done to a machine learning model? In short, I don't\nsee how a machine learning model trained for one task can be fixed to work on\na slightly different task. It would require a whole new retraining of the model\nevery time you want to make a change. Google even spoke about this issue in their\npaper [Machine Learning:\nThe High-Interest Credit Card of Technical Debt](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf). In our ever-changing world, you must truly\nconsider if you want to use a technology that requires so much effort even to change\nits purpose slightly.\n\n# Declarative Programming: is it Nirvana?\n\nI am a huge fan of declarative programming, whether from the [Wolfram Language](https://en.wikipedia.org/wiki/Wolfram_Language) or from [Prolog](https://en.wikipedia.org/wiki/Prolog).\nIn such a style, the programmer simply states the rules of the system formally, and\nthe language takes care of implementation concerns. This approach is easy to understand\nbecause it is just the rules of the system. In addition, it is easy to understand\nthe behavoir of a program: just follow which rules it applied.  If you have never\nheard of prolog, I strongly encourage you to check it out. It is unlike any other\nprogramming language, not part of the C or ALGOL lineage. It will rethink the way\nthat you understand the purpose of computers, [so try it out](https://swish.swi-prolog.org/).","date":"January 27, 2022","tags":["machine learning"," ml"," AI"],"title":"Machine Learning is overrated","slug":"machine-learning-overrated","description":"I don't understand the hype around machine learning","image":"/images/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg"},"reccomendedBlog":[{"content":"\nSandeep Srinivasan is a veteran of the machine learning industry. While most have gravitated towards it since the hype of the late 2000s, Srinivasan claims he has been “interested in machine learning since the 1990s,” almost noachian in this quickly evolving field. Today, I sat down with Srinivasan in order to glean what insights he has after pioneering this industry for 30 years. \n\nI am struck by Srinivasan’s positive attitude. He himself concurs, saying its necessary to be “a lifelong learner” to succeed in this field. Even with his experience, he is “still taking courses” to learn new skills and improve himself. His optimism extends further, however. When describing how he first got into machine learning, Srinivasan said a professor was too embarrassed that no one signed up for his course in machine learning, so he paid Srinivasan $10 an hour to be the only attendant. Despite being happenstance, Srinivasan tells the story with a smile as if it is a good joke. He brings this sense of optimism to his work, summarizing that the “best part of [his] job” is to meet bright 20-year-olds who have a “beginner’s mindset” and are eager to learn. \n\nFinding these bright 20-year-olds is becoming increasingly difficult for Srinivasan, unfortunately. In this white-hot field, he must muscle his way through the likes of “Google, Facebook, Amazon, and Apple.” Due to this drive for talent, Srinivasan complains that “the hardest part” of his job is hiring for his small machine learning startup.\n\nDespite his recruiting woes, Srinivasan claims he is not deterred about founding a small machine learning startup. He says the experience has been rewarding for him. When speaking on the advantages of founding a startup, he says he gets to “eat [his] own dog food,” in the sense that he is excited to try out all the products as a consumer that he is inventing. He emphasize that a founder must act as the first customer of a product, because if the founder doesn’t believe in the product, then neither will the clients. \n\nThe main area where Srinivasan eagerly gets to try out his new inventions is with his products aiming to “simplify machine learning.” He uses his own online dashboard to set up machine learning models in a few clicks, and soon he hopes others can do the same. His mission is to “democratize machine learning for multiple applications.”  In the future, Srinivasan imagines that doctors or other domain experts could create machine learning models without coding and without technical knowledge. \n\t\nDoctors are not the only profession Srinivasan predicts machine learning will impact. He thinks that many “repetitive tasks” such as “data entry” for taxes will disappear. In addition, manual tasks, such as image labeling will be automated away by machine learning.\n\nOverall, I am very impressed with Srinivasan’s accomplishments. He certainly has the air of someone well-informed on the industry, and he was able to use his knowledge to give me personal advice for the job market (“learn statistics”). Given how he effuses knowledge in every aspect of machine learning, I would not be surprised if his startup goes far. This experience was very informative for me, and I see myself possibly having a career in machine learning. I suggest to the reader to conduct a similar informational interview, in the hopes that one could learn about an intriguing field from a personality. \n\n\n \n","date":"Feb 27, 2022","tags":["machine learning"," interview"," coding"],"title":"Interview with Machine Learning Expert","slug":"sandeep-interview","description":"After his 30 years in the field, machine learning engineer Sandeep Srinivasan offers advice on how to tackle this complex career.","image":"/images/sandeep.jpg"},{"content":"\nBecause I program in haskell, I have a lot of experience with monads, and now I see monads wherever I go. Monads are truly everywhere. Many people have already interacted with monads without even realising it. In fact, Javascript's promise is a monad. Despite this, the first result on google search for \"is promise a monad\", is titled, \"no, promise is not a monad\". I see this blog post as a response to that article. In this post by Buzz De Cafe, the author claims that javascript's promise is not a monad because it does not follow the mathematical laws of monads. However, nobody in haskell actually uses moands in the mathematical sense. It doesn't really matter if Promise is a mathematical monad, what really matters is if it acts like a programming monad, which it does. For example, let's take the haskell function \n\n```haskell\n\nsequence :: (Traversable t, Monad m) =\u003e t (m a) -\u003e m (t a)\n\n```\nWhen I saw this, I was struck by how similar it is to Javascript's\n\n```haskell\nPromise.all ::[promise a] -\u003e promise [a]\n```\nI have written the type signature of Javascript's Promise.all in haskell syntax to lay apparent how similar these two functions are. Also let us look at .then vs \u003e\u003e=\n\n\n```haskell,\n\n (\u003e\u003e=) :: Monad m =\u003e m a -\u003e (a -\u003e m b) -\u003e m b\n```\n\n```javascript\n\nfetch(url)\n\t.then(data=\u003e data.json())\n```\nBoth haskell's bind operator \u003e\u003e= and javascript .then extract the internal value of a monad. And let me remind the reader, that \u003e\u003e= is the minimal operator for the monad typeclass in haskell, so if promise were defined in haskell, all one would have to do is\n\n```haskell\ninstance Monad Promise where\n     (\u003e\u003e=) = (then)\n```\n\nIt is obvious that Promise has all the characteristics of a Monad, so when haskellers complain that it does not obey some mathematical law, those are just sprinklings on top of a monad that don't really affect its functionality.  \n\nSome people may knock Javascript for not being as intellectual or high level as Haskell, but in fact it has almost all of the same structures, just with not as mathematical names. The example of Javascript's Promise being a monad shows that it is truly a high level language. \n\n\n\n","date":"April 12, 2021","tags":["haskell"],"title":"Why promise IS a monad","slug":"promise-is-a-monad","description":"There are monads all around us, even if we don't notice it","image":"/images/monad.jpg"},{"content":"\n\n\nI recently looked at [A Talk by Bob Martin](https://www.youtube.com/watch?v=P2yr-3F6PQo) in which\nhe says that we've \"basically discovered all the programming paradigms\". He says this due to his\nview that programming paradigms are just restrictions on what programmers can do, but I disagree.\nMy view is that **a good programming language is the minimal amount needed to add to a model\nof computation**. I will explain what I mean.\n\n## Models of Computation\nAccording to Wikipedia, a [Model of Computation](https://en.wikipedia.org/wiki/Model_of_computation) is\n\"a model which describes how an output of a mathematical function is computed given an input\".\nThere are a list of models of computation on the page, which include\n- Post machines\n- Pushdown automata\n- Register machines\n- Random-access machines\n- Turing machines\n- Decision tree model\n- Abstract rewriting systems\n- Combinatory logic\n- General recursive functions\n- Lambda calculus\n- Actor model\n- Cellular automaton\n- Interaction nets\n- Kahn process networks\n- Logic gates and digital circuits\n- Petri nets\n- Synchronous Data Flow\n\nAs you can see, there are many models of computation.\n## Minimal Addition\nWhat do I mean by \"minimal additon\". Well, a model of computation only describes mathematical functions, which are pure. We would like our programs to be able to interact with the outside world. This means\nsome abilities must be added to a model of computation in order to allow interaction. The gold\nstandard for this is [Haskell](https://en.wikipedia.org/wiki/Haskell), with its monadic model of computation. In a good language, effects should be modelled using some kind of language construct, such\nas Haskell's monads, but I think the construct will vary for each model of computation.\n\n## Examples of Well-Loved Languages\n\nI will show how the best-designed languages today are simply the minimal additions to already good\nmodels of computation.\n\n\n| Model of Computation| Language|\n|--------------------|----------|\n| Untyped Lambda Calculus | Lisp|\n| Typed Lambda Calculus | Haskell|\n| Predicate Logic | Prolog |\n| Rewriting System | Wolfram |\n| Actor Model | Smalltalk |\n| Interaction Nets | [Kind2](https://github.com/Kindelia/Kind2) |\n| Combinatory Logic | APL | \n\nHopefully you see that all the languages on the right column are widely admired for being well-designed.\nAnd what unifies them all? They are all strictly based on a model of computation.\n\nLet us discuss the example of Lisp. Lisp is the [minimal wrapper](https://news.ycombinator.com/item?id=29950782) on top of the untyped lambda calculus. It does not include any features\nbeyound unleashing the inherent power of the lambda calculus. This is the main difference\nbetween Smalltalk and Java. Both are based on the [Actor Model](https://en.wikipedia.org/wiki/Actor_model)\nbut Smalltalk sticks truthfully to its model of computation, while Java confusedly\nborrows concepts from other languages.\n\n## Turing Completeness\nDue to Turing Completeness, we can say that all models of computation are equally expressive.\nSo, it remains to be decided what model we should use. I think we should first choose a model\nthat is the best for our needs.\n\n## Good Models of Computation\nGiven all these models of computation, which should we chose as a basis for our languages?\nI think it comes down to 2 critera:\n- being simple\n- having nice properties\n\nFor the first point, I think the model should be very simple. I think the [SK Calculus](https://en.wikipedia.org/wiki/SKI_combinator_calculus) is very good in this regard, and it might be the simplest model\nof computation that I have ever seen. Turing Machines, although the most well-known example of\na model of computation, are actually very complex and hard to explain. \n\nFor the second point, I mean that the model has properties that make it easy to implement and\nfast to run on our given hardware. One model that is very promising is [Interaction Nets](https://en.wikipedia.org/wiki/Interaction_nets#Properties) because they have these properties\n\n- locality (only active pairs can be rewritten)\n- linearity (each interaction rule can be applied in constant time)\n- strong confluence \n\nThese properties would make an implementation very fast, so I encourage you to try writing\nan implementation.\n\n\n## Advice to Aspiring Language Designers\nIf you want to design a language, I have some advice for how to do it. The first step is to\nchoose a model of computation that has not yet been well-explored. For example, I would\nnot design a language based on lambda calculus because that space has already been perfected\nby Lisp and Haskell, and I would not design a langauge based on predicate logic because\nProlog already does a good job there. I would choose a model that has good properties (can\nbe executed quickly on hardware) and has not been explored by any other language. Right\nnow I think the field of Linear Logic doesn't have its own industry-strength programming\nlanguage. If you think you can design such a language, I would encourage you on.\n\n\n## Summary\nIn short, a programming language is nothing but an implentation of a model of computation.","date":"Oct 10, 2022","tags":["programming"," math"," computation"," lisp"],"title":"How to Create an Excellent Programming Lanugage","slug":"create-language","description":"I have come up with a method to design well-loved languages","image":"/images/lisp.png"}]},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"machine-learning-overrated"},"buildId":"_0_1wzyqbB4XLdIGrVw4s","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>